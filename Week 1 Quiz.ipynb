{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For questions 1 to 3:\n",
    "\n",
    "Perform a linear regression to predict Y from X1, X2, and X3. Use the file homework_1.1.csv.\n",
    "\n",
    "\n",
    "Which of the following is closest to the coefficient of X1? \n",
    "\n",
    "Option A\n",
    "3\n",
    "\n",
    "Option B\n",
    "1\n",
    "\n",
    "\n",
    "\n",
    "Which Xi has the greatest difference between the amount Y increases for each 1 unit of Xi (fixing the other Xi’s), as opposed to the amount that Y increases for each 1 unit of Xi in the dataset, on average (not fixing the other Xis)? Hint: for the former, you'll have to regress Y on Xi alone, while for the latter, you'll have to regress Y on all three Xis. \n",
    "\n",
    "Option A\n",
    "X2\n",
    "\n",
    "Option B\n",
    "X3\n",
    "\n",
    "Option C\n",
    "X1\n",
    "\n",
    "\n",
    "Question 3\n",
    "6\n",
    " Points\n",
    "Question 3\n",
    "When regressing Y on all Xis together, which coefficient is most significant, considering the t-statistic as a measure of significance? \n",
    "\n",
    "Option A\n",
    "X1 \n",
    "\n",
    "Option B\n",
    "X2 \n",
    "\n",
    "Option C\n",
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (1.16.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [statsmodels]\u001b[0m [statsmodels]\n",
      "\u001b[1A\u001b[2KSuccessfully installed patsy-1.0.1 statsmodels-0.14.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.991\n",
      "Model:                            OLS   Adj. R-squared:                  0.991\n",
      "Method:                 Least Squares   F-statistic:                 3.543e+04\n",
      "Date:                Sun, 14 Sep 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:25:58   Log-Likelihood:                -727.62\n",
      "No. Observations:                1000   AIC:                             1463.\n",
      "Df Residuals:                     996   BIC:                             1483.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0026      0.016      0.166      0.868      -0.029       0.034\n",
      "X1             1.0071      0.017     60.984      0.000       0.975       1.040\n",
      "X2             1.9646      0.037     53.283      0.000       1.892       2.037\n",
      "X3             2.9755      0.015    196.645      0.000       2.946       3.005\n",
      "==============================================================================\n",
      "Omnibus:                        0.655   Durbin-Watson:                   1.960\n",
      "Prob(Omnibus):                  0.721   Jarque-Bera (JB):                0.592\n",
      "Skew:                          -0.058   Prob(JB):                        0.744\n",
      "Kurtosis:                       3.029   Cond. No.                         5.94\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Coefficients:\n",
      "const    0.002643\n",
      "X1       1.007138\n",
      "X2       1.964569\n",
      "X3       2.975489\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"homework_1.1.csv\")\n",
    "\n",
    "# Separate predictors and target\n",
    "X = df[[\"X1\", \"X2\", \"X3\"]]\n",
    "y = df[\"Y\"]\n",
    "\n",
    "# Add constant (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Show results\n",
    "print(model.summary())\n",
    "\n",
    "# If you just want the coefficients directly:\n",
    "print(\"Coefficients:\")\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const    0.002643\n",
       "X1       1.007138\n",
       "X2       1.964569\n",
       "X3       2.975489\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[[\"X1\", \"X2\", \"X3\"]]\n",
    "y = df[\"Y\"]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 simple regression coefficient: 1.841761099171514\n",
      "X2 simple regression coefficient: 4.083612579423011\n",
      "X3 simple regression coefficient: 3.097041202043796\n"
     ]
    }
   ],
   "source": [
    "for col in [\"X1\", \"X2\", \"X3\"]:\n",
    "    X_single = sm.add_constant(df[[col]])\n",
    "    model_single = sm.OLS(y, X_single).fit()\n",
    "    print(f\"{col} simple regression coefficient: {model_single.params[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const    0.002643\n",
      "X1       1.007138\n",
      "X2       1.964569\n",
      "X3       2.975489\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = sm.add_constant(df[[\"X1\", \"X2\", \"X3\"]])\n",
    "model_multi = sm.OLS(y, X).fit()\n",
    "print(model_multi.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 3 - When regressing Y on all Xis together, which coefficient is most significant, considering the t-statistic as a measure of significance? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.991\n",
      "Model:                            OLS   Adj. R-squared:                  0.991\n",
      "Method:                 Least Squares   F-statistic:                 3.543e+04\n",
      "Date:                Sun, 14 Sep 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:33:16   Log-Likelihood:                -727.62\n",
      "No. Observations:                1000   AIC:                             1463.\n",
      "Df Residuals:                     996   BIC:                             1483.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0026      0.016      0.166      0.868      -0.029       0.034\n",
      "X1             1.0071      0.017     60.984      0.000       0.975       1.040\n",
      "X2             1.9646      0.037     53.283      0.000       1.892       2.037\n",
      "X3             2.9755      0.015    196.645      0.000       2.946       3.005\n",
      "==============================================================================\n",
      "Omnibus:                        0.655   Durbin-Watson:                   1.960\n",
      "Prob(Omnibus):                  0.721   Jarque-Bera (JB):                0.592\n",
      "Skew:                          -0.058   Prob(JB):                        0.744\n",
      "Kurtosis:                       3.029   Cond. No.                         5.94\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X = sm.add_constant(df[[\"X1\", \"X2\", \"X3\"]])\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Question 4 and 5: Use NearestNeighbors to match data based on variables Z, given the file homework_1.2.csv. Pick the best match in X = 0 corresponding to each X = 1. Use the Z values to perform the match: a good match with X = 1 is the item whose Z value is closest to the given sample's Z value with X = 0. I suggest using sklearn's NearestNeighbors to do this, but there are many ways to do it. \n",
    "\n",
    "Question 4 \n",
    "\n",
    "What is the distance of the farthest match in this set? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df2 = pd.read_csv(\"homework_1.2.csv\")   # columns: X, Y, Z\n",
    "\n",
    "# Split into treatment (X=1) and control (X=0)\n",
    "treat = df2[df2[\"X\"] == 1].reset_index(drop=True)\n",
    "ctrl  = df2[df2[\"X\"] == 0].reset_index(drop=True)\n",
    "\n",
    "# Arrays for matching\n",
    "Z_treat = treat[[\"Z\"]].values\n",
    "Z_ctrl  = ctrl[[\"Z\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 - Farthest match distance: 0.2102170871093757\n"
     ]
    }
   ],
   "source": [
    "# Fit NearestNeighbors on control (X=0)\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm=\"auto\").fit(Z_ctrl)\n",
    "\n",
    "# Find best match in control for each treatment Z\n",
    "distances, indices = nn.kneighbors(Z_treat)\n",
    "\n",
    "# Farthest match distance (max over all matches)\n",
    "farthest_match = float(distances.max())\n",
    "print(\"Q4 - Farthest match distance:\", farthest_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5 What is the effect? (The difference between the average Y value for X = 0 values vs. the average Y value for X = 1, where the X = 0 sample has the best match for each X = 1 value). So we use the matched sample of X = 0 and the full sample of X = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 - Treatment effect: 0.5433600652185839\n"
     ]
    }
   ],
   "source": [
    "# Get matched control rows corresponding to each treatment\n",
    "matched_ctrl = ctrl.iloc[indices.flatten()].reset_index(drop=True)\n",
    "\n",
    "# Average Y in treatment group\n",
    "avg_treat = float(treat[\"Y\"].mean())\n",
    "\n",
    "# Average Y in matched control group\n",
    "avg_ctrl_matched = float(matched_ctrl[\"Y\"].mean())\n",
    "\n",
    "# Effect = difference\n",
    "effect = avg_treat - avg_ctrl_matched\n",
    "print(\"Q5 - Treatment effect:\", effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For questions 6 and 7: Use NearestNeighbors to match data based on variables Z, given the file homework_1.2.csv. Try approach B: Pick all of the matches in X = 0 that are within a distance 0.2 of each X = 1. Duplicates are okay, in case a given sample with X = 0 is a good match for multiple items with X = 1. \n",
    "\n",
    "\n",
    "6) How many duplicates do you end up with? (Count all but the first duplicate in each group. One way to do this is to use radius_neighbors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split treatment (X=1) and control (X=0)\n",
    "treat = df2[df2[\"X\"] == 1].reset_index(drop=True)\n",
    "ctrl  = df2[df2[\"X\"] == 0].reset_index(drop=True)\n",
    "\n",
    "# Arrays for matching on Z\n",
    "Z_treat = treat[[\"Z\"]].values\n",
    "Z_ctrl  = ctrl[[\"Z\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows total=100, treat=48, ctrl=52\n",
      "Shapes -> Z_treat: (48, 1) Z_ctrl: (52, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "# Load data\n",
    "df2 = pd.read_csv(\"homework_1.2.csv\")\n",
    "\n",
    "# Make sure columns exist and are numeric\n",
    "expected_cols = {\"X\",\"Y\",\"Z\"}\n",
    "missing = expected_cols - set(df2.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns: {missing}. Found columns: {list(df2.columns)}\")\n",
    "\n",
    "for c in [\"X\",\"Y\",\"Z\"]:\n",
    "    df2[c] = pd.to_numeric(df2[c], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with NaNs in needed cols\n",
    "df2 = df2.dropna(subset=[\"X\",\"Y\",\"Z\"]).reset_index(drop=True)\n",
    "\n",
    "# Split groups\n",
    "treat = df2[df2[\"X\"] == 1].reset_index(drop=True)\n",
    "ctrl  = df2[df2[\"X\"] == 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Rows total={len(df2)}, treat={len(treat)}, ctrl={len(ctrl)}\")\n",
    "\n",
    "if len(treat) == 0 or len(ctrl) == 0:\n",
    "    raise ValueError(\"Need both treatment (X=1) and control (X=0) rows to proceed.\")\n",
    "\n",
    "# Arrays for matching (1D feature: Z)\n",
    "Z_treat = treat[[\"Z\"]].values\n",
    "Z_ctrl  = ctrl[[\"Z\"]].values\n",
    "\n",
    "print(\"Shapes -> Z_treat:\", Z_treat.shape, \"Z_ctrl:\", Z_ctrl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6 - Number of duplicates (radius=0.2): 685\n",
      "Control rows reused (>1 use): {np.int64(0): 21, np.int64(1): 21, np.int64(2): 21, np.int64(4): 21, np.int64(5): 19, np.int64(9): 28, np.int64(12): 21, np.int64(17): 20, np.int64(20): 21, np.int64(21): 22, np.int64(25): 22, np.int64(31): 22, np.int64(35): 25, np.int64(41): 19, np.int64(43): 21, np.int64(45): 19, np.int64(48): 23, np.int64(15): 19, np.int64(27): 19, np.int64(34): 20, np.int64(3): 17, np.int64(13): 16, np.int64(14): 16, np.int64(18): 17, np.int64(26): 15, np.int64(28): 15, np.int64(37): 16, np.int64(39): 16, np.int64(42): 16, np.int64(46): 16, np.int64(22): 12, np.int64(24): 12, np.int64(49): 11, np.int64(6): 5, np.int64(7): 5, np.int64(8): 5, np.int64(10): 8, np.int64(11): 8, np.int64(16): 5, np.int64(19): 5, np.int64(23): 7, np.int64(29): 8, np.int64(30): 7, np.int64(32): 7, np.int64(33): 6, np.int64(36): 5, np.int64(38): 8, np.int64(40): 5, np.int64(44): 6, np.int64(47): 8, np.int64(50): 5, np.int64(51): 5}\n"
     ]
    }
   ],
   "source": [
    "# Fit radius neighbors on control\n",
    "radius = 0.2\n",
    "nn = NearestNeighbors(radius=radius, algorithm=\"auto\")\n",
    "nn.fit(Z_ctrl)\n",
    "\n",
    "# Find neighbors for each treatment point\n",
    "dist_lists, idx_lists = nn.radius_neighbors(Z_treat, return_distance=True)\n",
    "\n",
    "# Flatten all matched control indices (some groups may be empty)\n",
    "all_ctrl_indices = np.concatenate(idx_lists) if len(idx_lists) and any(len(ix)>0 for ix in idx_lists) else np.array([], dtype=int)\n",
    "\n",
    "# Count reuse of control indices\n",
    "counts = Counter(all_ctrl_indices)\n",
    "duplicates = sum(max(0, c - 1) for c in counts.values())\n",
    "print(f\"Q6 - Number of duplicates (radius={radius}):\", duplicates)\n",
    "\n",
    "# (Optional) show which control rows were reused\n",
    "reused = {k:v for k,v in counts.items() if v > 1}\n",
    "if reused:\n",
    "    print(\"Control rows reused (>1 use):\", reused)\n",
    "else:\n",
    "    print(\"No control rows reused.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) What is the effect? (Note: to compute the effect, you should take the mean of the Y values in each neighbor group, then average the Y for each group.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "radius = 0.2\n",
    "\n",
    "# Make sure we have Z arrays\n",
    "Z_treat = treat[[\"Z\"]].values\n",
    "Z_ctrl  = ctrl[[\"Z\"]].values\n",
    "\n",
    "# Radius neighbors on control\n",
    "nn = NearestNeighbors(radius=radius, algorithm=\"auto\").fit(Z_ctrl)\n",
    "dist_lists, idx_lists = nn.radius_neighbors(Z_treat, return_distance=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ctrl_means = []\n",
    "matched_treat_Y  = []\n",
    "\n",
    "for i, inds in enumerate(idx_lists):\n",
    "    # inds = indices into control rows that are within radius of treat[i] in Z\n",
    "    if len(inds) == 0:\n",
    "        continue  # skip treatment rows with no control neighbors\n",
    "\n",
    "    ctrl_mean_y = ctrl.iloc[inds][\"Y\"].mean()\n",
    "    group_ctrl_means.append(ctrl_mean_y)\n",
    "    matched_treat_Y.append(treat.iloc[i][\"Y\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q7 - Treatment effect (matched groups): 0.5688516534127853\n",
      "  Groups matched: 46\n",
      "  Avg Y (treat, matched groups): 1.1100363138620082\n",
      "  Avg of group control means: 0.5411846604492229\n"
     ]
    }
   ],
   "source": [
    "group_ctrl_means = np.array(group_ctrl_means, dtype=float)\n",
    "matched_treat_Y  = np.array(matched_treat_Y, dtype=float)\n",
    "\n",
    "if matched_treat_Y.size == 0:\n",
    "    raise ValueError(f\"No treatment rows had any control neighbors within radius={radius}. Try a larger radius (e.g., 0.3).\")\n",
    "\n",
    "avg_ctrl  = group_ctrl_means.mean()    # average of group control means\n",
    "avg_treat = matched_treat_Y.mean()     # average Y over matched treatment rows\n",
    "effect = avg_treat - avg_ctrl\n",
    "\n",
    "print(\"Q7 - Treatment effect (matched groups):\", effect)\n",
    "print(\"  Groups matched:\", matched_treat_Y.size)\n",
    "print(\"  Avg Y (treat, matched groups):\", avg_treat)\n",
    "print(\"  Avg of group control means:\", avg_ctrl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
